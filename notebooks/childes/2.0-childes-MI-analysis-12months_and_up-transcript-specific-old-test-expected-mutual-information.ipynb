{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:13:35.957079Z",
     "start_time": "2020-01-08T06:13:35.917513Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:13:38.181682Z",
     "start_time": "2020-01-08T06:13:36.047523Z"
    }
   },
   "outputs": [],
   "source": [
    "from childes_mi.utils.paths import CHILDES_DFS, DATA_DIR\n",
    "from childes_mi.utils.general import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-08T06:13:36.174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-08T06:13:36.303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df = pd.read_pickle(CHILDES_DFS/'childes_df_subset.pickle')\n",
    "#transcript_df['dataset'] = [i[0].split('/')[11] for i in transcript_df.transcript_xml.values]\n",
    "len(transcript_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-08T06:13:36.437Z"
    }
   },
   "outputs": [],
   "source": [
    "# mask for only a subset of corpuses\n",
    "possible_corpuses = [\n",
    "    # carterette is also transcribed in IPA\n",
    "    \"Carterette\",  # 1st, 3rd, 5th grade, adults, speech naturalistic speech in IPA https://childes.talkbank.org/access/Eng-NA/Carterette.html\n",
    "    \"Gillam\",  # normal subset, spontaneous storytelling 5-11y11m https://childes.talkbank.org/access/Clinical-MOR/Gillam.html\n",
    "    \"Braunwald\",  # single subject from 1-6yo talking to family https://childes.talkbank.org/access/Eng-NA/Braunwald.html\n",
    "    \"Brown\",  # 1y6m-5y1m, 3 subjects naturalistic talking to parents https://childes.talkbank.org/access/Eng-NA/Brown.html\n",
    "    \"EllisWeismer\",  # 2y6m - 5y6 naturalistic play-oriented talk with instructor # https://childes.talkbank.org/access/Clinical-MOR/EllisWeismer.html\n",
    "    \"Gleason\",  # 2;1 - 5;2 family interactions https://childes.talkbank.org/access/Eng-NA/Gleason.html\n",
    "    \"NH\",# Nicholas-NH,  # 1;0-4;0, videotaped play with parent https://childes.talkbank.org/access/Clinical-MOR/Nicholas/NH.html\n",
    "    \"Post\",  # 1;7-2;8 60m session with family and toys https://childes.talkbank.org/access/Eng-NA/Post.html\n",
    "    \"Normal\",  #  Rondal-Normal 3-12yo free play with mother and child https://childes.talkbank.org/access/Clinical-MOR/Normal /Normal.html\n",
    "]\n",
    "corpus_mask = [corpus in possible_corpuses for corpus in transcript_df.corpus.values]\n",
    "transcript_df = transcript_df[corpus_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-08T06:13:36.582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask for a subset of roles\n",
    "possible_roles = ['Target_Child', 'Child']\n",
    "role_mask = [role in possible_roles for role in transcript_df.role.values]\n",
    "transcript_df = transcript_df[role_mask]\n",
    "len(transcript_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:34.959378Z",
     "start_time": "2020-01-08T05:51:34.577583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>lang</th>\n",
       "      <th>corpus</th>\n",
       "      <th>date</th>\n",
       "      <th>participant_key</th>\n",
       "      <th>role</th>\n",
       "      <th>age</th>\n",
       "      <th>language</th>\n",
       "      <th>sex</th>\n",
       "      <th>POS</th>\n",
       "      <th>words</th>\n",
       "      <th>morphemes</th>\n",
       "      <th>transcript_xml</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_morphemes</th>\n",
       "      <th>n_POS</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72301</th>\n",
       "      <td>11312/c-00015848-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P6Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[pro:per, v, , , conj, n:prop, coord, pro:ind...</td>\n",
       "      <td>[[you, mean, uh, um, like, England, or, someth...</td>\n",
       "      <td>[[you, mean, uh, um, like, England, or, someth...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>19903</td>\n",
       "      <td>2102</td>\n",
       "      <td>2102</td>\n",
       "      <td>72.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72302</th>\n",
       "      <td>11312/c-00015847-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P10Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[co, , pro:sub, v, , det:num, n], [coord, , p...</td>\n",
       "      <td>[[well, um, I, have, uh, one, sister], [and, u...</td>\n",
       "      <td>[[well, um, I, have, uh, one, sister], [and, u...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>26941</td>\n",
       "      <td>2729</td>\n",
       "      <td>2729</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72303</th>\n",
       "      <td>11312/c-00015849-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P8Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[v, n, n], [co, n:prop, v, prep, det:art, n, ...</td>\n",
       "      <td>[[lets, watch, tv], [well, Nancy, said, for, t...</td>\n",
       "      <td>[[let-3S, watch, tv], [well, Nancy, say-PAST, ...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>21419</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PID lang      corpus        date participant_key  \\\n",
       "72301  11312/c-00015848-1  eng  Carterette  1984-01-01             CHI   \n",
       "72302  11312/c-00015847-1  eng  Carterette  1984-01-01             CHI   \n",
       "72303  11312/c-00015849-1  eng  Carterette  1984-01-01             CHI   \n",
       "\n",
       "               role   age language  sex  \\\n",
       "72301  Target_Child   P6Y      eng  NaN   \n",
       "72302  Target_Child  P10Y      eng  NaN   \n",
       "72303  Target_Child   P8Y      eng  NaN   \n",
       "\n",
       "                                                     POS  \\\n",
       "72301  [[pro:per, v, , , conj, n:prop, coord, pro:ind...   \n",
       "72302  [[co, , pro:sub, v, , det:num, n], [coord, , p...   \n",
       "72303  [[v, n, n], [co, n:prop, v, prep, det:art, n, ...   \n",
       "\n",
       "                                                   words  \\\n",
       "72301  [[you, mean, uh, um, like, England, or, someth...   \n",
       "72302  [[well, um, I, have, uh, one, sister], [and, u...   \n",
       "72303  [[lets, watch, tv], [well, Nancy, said, for, t...   \n",
       "\n",
       "                                               morphemes  \\\n",
       "72301  [[you, mean, uh, um, like, England, or, someth...   \n",
       "72302  [[well, um, I, have, uh, one, sister], [and, u...   \n",
       "72303  [[let-3S, watch, tv], [well, Nancy, say-PAST, ...   \n",
       "\n",
       "                                          transcript_xml  n_words  \\\n",
       "72301  [/mnt/cube/tsainbur/Projects/github_repos/chil...    19903   \n",
       "72302  [/mnt/cube/tsainbur/Projects/github_repos/chil...    26941   \n",
       "72303  [/mnt/cube/tsainbur/Projects/github_repos/chil...    21419   \n",
       "\n",
       "       n_morphemes  n_POS  age_months  age_years  \n",
       "72301         2102   2102        72.0        6.0  \n",
       "72302         2729   2729       120.0       10.0  \n",
       "72303         2019   2019        96.0        8.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:35.000277Z",
     "start_time": "2020-01-08T05:51:34.965629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carterette' 'Brown' 'Braunwald' 'Post' 'Gleason' 'Normal' 'Gillam'\n",
      " 'EllisWeismer' 'NH']\n"
     ]
    }
   ],
   "source": [
    "print(transcript_df.corpus.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:35.101384Z",
     "start_time": "2020-01-08T05:51:35.003715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1247296 words in this dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"there are {} words in this dataset\".format(np.sum(transcript_df.n_words.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get child ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:35.183710Z",
     "start_time": "2020-01-08T05:51:35.104642Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_age(age_year):\n",
    "    \"Caclculate age in months from a string in CHILDES format\"\n",
    "    if age_year[-1] == 'Y':\n",
    "        age_year += '0'\n",
    "    m = re.match(\"P(\\d+)Y(\\d+)M?(\\d?\\d?)D?\", age_year)\n",
    "    age_month = int(m.group(1)) * 12 + int(m.group(2))\n",
    "    try:\n",
    "        if int(m.group(3)) > 15:\n",
    "            age_month += 1\n",
    "    # some corpora don't have age information?\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    return age_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:35.757279Z",
     "start_time": "2020-01-08T05:51:35.186484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd71da623fb74842a66e4c19df9c4121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2785), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transcript_df[\"age_months\"] = [\n",
    "    convert_age(i) if type(i) == str else np.nan for i in tqdm(transcript_df.age.values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:36.220895Z",
     "start_time": "2020-01-08T05:51:35.759728Z"
    }
   },
   "outputs": [],
   "source": [
    "transcript_df[\"age_years\"] = transcript_df[\"age_months\"] / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:51:37.609363Z",
     "start_time": "2020-01-08T05:51:37.223576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>lang</th>\n",
       "      <th>corpus</th>\n",
       "      <th>date</th>\n",
       "      <th>participant_key</th>\n",
       "      <th>role</th>\n",
       "      <th>age</th>\n",
       "      <th>language</th>\n",
       "      <th>sex</th>\n",
       "      <th>POS</th>\n",
       "      <th>words</th>\n",
       "      <th>morphemes</th>\n",
       "      <th>transcript_xml</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_morphemes</th>\n",
       "      <th>n_POS</th>\n",
       "      <th>age_months</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72301</th>\n",
       "      <td>11312/c-00015848-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P6Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[pro:per, v, , , conj, n:prop, coord, pro:ind...</td>\n",
       "      <td>[[you, mean, uh, um, like, England, or, someth...</td>\n",
       "      <td>[[you, mean, uh, um, like, England, or, someth...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>19903</td>\n",
       "      <td>2102</td>\n",
       "      <td>2102</td>\n",
       "      <td>72.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72302</th>\n",
       "      <td>11312/c-00015847-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P10Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[co, , pro:sub, v, , det:num, n], [coord, , p...</td>\n",
       "      <td>[[well, um, I, have, uh, one, sister], [and, u...</td>\n",
       "      <td>[[well, um, I, have, uh, one, sister], [and, u...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>26941</td>\n",
       "      <td>2729</td>\n",
       "      <td>2729</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72303</th>\n",
       "      <td>11312/c-00015849-1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Carterette</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>CHI</td>\n",
       "      <td>Target_Child</td>\n",
       "      <td>P8Y</td>\n",
       "      <td>eng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[v, n, n], [co, n:prop, v, prep, det:art, n, ...</td>\n",
       "      <td>[[lets, watch, tv], [well, Nancy, said, for, t...</td>\n",
       "      <td>[[let-3S, watch, tv], [well, Nancy, say-PAST, ...</td>\n",
       "      <td>[/mnt/cube/tsainbur/Projects/github_repos/chil...</td>\n",
       "      <td>21419</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PID lang      corpus        date participant_key  \\\n",
       "72301  11312/c-00015848-1  eng  Carterette  1984-01-01             CHI   \n",
       "72302  11312/c-00015847-1  eng  Carterette  1984-01-01             CHI   \n",
       "72303  11312/c-00015849-1  eng  Carterette  1984-01-01             CHI   \n",
       "\n",
       "               role   age language  sex  \\\n",
       "72301  Target_Child   P6Y      eng  NaN   \n",
       "72302  Target_Child  P10Y      eng  NaN   \n",
       "72303  Target_Child   P8Y      eng  NaN   \n",
       "\n",
       "                                                     POS  \\\n",
       "72301  [[pro:per, v, , , conj, n:prop, coord, pro:ind...   \n",
       "72302  [[co, , pro:sub, v, , det:num, n], [coord, , p...   \n",
       "72303  [[v, n, n], [co, n:prop, v, prep, det:art, n, ...   \n",
       "\n",
       "                                                   words  \\\n",
       "72301  [[you, mean, uh, um, like, England, or, someth...   \n",
       "72302  [[well, um, I, have, uh, one, sister], [and, u...   \n",
       "72303  [[lets, watch, tv], [well, Nancy, said, for, t...   \n",
       "\n",
       "                                               morphemes  \\\n",
       "72301  [[you, mean, uh, um, like, England, or, someth...   \n",
       "72302  [[well, um, I, have, uh, one, sister], [and, u...   \n",
       "72303  [[let-3S, watch, tv], [well, Nancy, say-PAST, ...   \n",
       "\n",
       "                                          transcript_xml  n_words  \\\n",
       "72301  [/mnt/cube/tsainbur/Projects/github_repos/chil...    19903   \n",
       "72302  [/mnt/cube/tsainbur/Projects/github_repos/chil...    26941   \n",
       "72303  [/mnt/cube/tsainbur/Projects/github_repos/chil...    21419   \n",
       "\n",
       "       n_morphemes  n_POS  age_months  age_years  \n",
       "72301         2102   2102        72.0        6.0  \n",
       "72302         2729   2729       120.0       10.0  \n",
       "72303         2019   2019        96.0        8.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:54:24.271464Z",
     "start_time": "2020-01-08T05:54:24.215413Z"
    }
   },
   "outputs": [],
   "source": [
    "age_cohorts = [[1, 1.5], [1.5, 2], [2, 2.5], [2.5, 3], [3, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:54:54.735802Z",
     "start_time": "2020-01-08T05:54:24.596667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755fe8e0489c4170a5dbc8719052144c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5 38.0 15154 5439\n",
      "1.5 2 98.0 57580 19538\n",
      "2 2.5 204.0 167722 50039\n",
      "2.5 3 121.5 110580 33211\n",
      "3 20 394.0 855307 249731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(age_cohort_low, age_cohort_high, np.median([len(i) for i in cohort_words]), n_words, n_unique_words)\n",
    "1 1.5 38.0 15154 5439\n",
    "1.5 2 98.0 57580 19538\n",
    "2 2.5 204.0 167722 50039\n",
    "2.5 3 121.5 110580 33211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MI\n",
    "- for each sequence, calculate the decay of a sequence as a function of distance between elements for that sequence indiividually\n",
    "- Treat each phoneme as unique for that child, and compute MI across children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:59:06.029505Z",
     "start_time": "2020-01-08T05:59:05.969808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.cluster.supervised module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics.cluster. Anything that cannot be imported from sklearn.metrics.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from childes_mi.information_theory import mutual_information as mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run MI by age cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T05:59:23.380510Z",
     "start_time": "2020-01-08T05:59:23.332260Z"
    }
   },
   "outputs": [],
   "source": [
    "distances = np.arange(1,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:06:01.753625Z",
     "start_time": "2020-01-08T05:59:24.250386Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb1ad142aee4b3680b51e65dbf7ab8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154 5439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f003898161234c888175bdf295a12616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMI reduced memory parallel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195cf82ae5c4d8f9dc4284c3bf152a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='compute emi', max=5372, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02389203129531103\n",
      "EMI reduced memory parallel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a0dc0a72a346738534e330622f213b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='compute emi', max=5290, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3ace2c933f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     (MI, MI_var), (shuff_MI, shuff_MI_var) = mi.sequential_mutual_information(\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcohort_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/childes_mi_project/childes_mi/information_theory/mutual_information.py\u001b[0m in \u001b[0;36msequential_mutual_information\u001b[0;34m(sequences, distances, n_jobs, verbosity, n_shuff_repeats, estimate, disable_tqdm, prefer, unclustered_element, **mi_kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             )\n\u001b[1;32m    290\u001b[0m             for dist_i, dist in enumerate(\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m         ]\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/childes_mi_project/childes_mi/information_theory/mutual_information.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmi_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             )\n\u001b[0;32m--> 290\u001b[0;31m             for dist_i, dist in enumerate(\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/childes_mi_project/childes_mi/information_theory/mutual_information.py\u001b[0m in \u001b[0;36mMI_from_distributions\u001b[0;34m(sequences, dist, estimate, unclustered_element, use_sklearn, adjusted_mi, **mi_kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m#emi = expected_mutual_information(C, n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0memi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memi_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                 \u001b[0;31m#emi = expected_mutual_information(C, n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, emi2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/childes_mi_project/childes_mi/information_theory/expected_mutual_information.py\u001b[0m in \u001b[0;36memi_parallel\u001b[0;34m(contingency, n_samples)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mgln_nij\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             )\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"compute emi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmininterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mi_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"MI\",\n",
    "        \"MI_var\",\n",
    "        \"shuff_MI\",\n",
    "        \"shuff_MI_var\",\n",
    "        \"distances\",\n",
    "        \"age_cohort_low\",\n",
    "        \"age_cohort_high\",\n",
    "        \"n_words\",\n",
    "        \"n_unique_words\",\n",
    "    ]\n",
    ")\n",
    "# for each age cohort\n",
    "for (age_cohort_low, age_cohort_high) in tqdm(age_cohorts):\n",
    "    cohort_subset = transcript_df[\n",
    "        (transcript_df.age_months/12 > age_cohort_low) & (transcript_df.age_months/12 <= age_cohort_high)\n",
    "    ]\n",
    "    \n",
    "    cohort_words = [[row.PID + \"_\" + i for i in list(flatten(row.words)) if i not in [None, 'xxx']] for idx, row in cohort_subset.iterrows()]\n",
    "    #cohort_words = [[i for i in list(flatten(row.words)) if i not in [None, 'xxx']] for idx, row in cohort_subset.iterrows()]\n",
    "    \n",
    "    n_words = len(np.concatenate(cohort_words))\n",
    "    n_unique_words = len(np.unique(np.concatenate(cohort_words)))\n",
    "    print(n_words, n_unique_words)\n",
    "    \n",
    "    \n",
    "    (MI, MI_var), (shuff_MI, shuff_MI_var) = mi.sequential_mutual_information(\n",
    "        cohort_words, distances=distances, n_jobs=1, estimate=False, prefer = None, verbosity=49\n",
    "    )\n",
    "    \n",
    "    \n",
    "    fig,axs = plt.subplots(ncols=2, figsize = (10,4))\n",
    "    ax = axs[0]\n",
    "    ax.scatter(distances, MI-shuff_MI)\n",
    "    ax.plot(distances, MI-shuff_MI, alpha = 0)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.scatter(distances, MI)\n",
    "    ax.scatter(distances, shuff_MI)\n",
    "    plt.show()\n",
    "    \n",
    "    mi_df.loc[len(mi_df)] = [\n",
    "        MI,\n",
    "        MI_var,\n",
    "        shuff_MI,\n",
    "        shuff_MI_var,\n",
    "        distances,\n",
    "        age_cohort_low,\n",
    "        age_cohort_high,\n",
    "        n_words,\n",
    "        n_unique_words,\n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cohort_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [[i for i in list(flatten(row.words)) if i not in [None, 'xxx']] for idx, row in cohort_subset.iterrows()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['more',\n",
       " 'cookie',\n",
       " 'more',\n",
       " 'cookie',\n",
       " 'more',\n",
       " 'juice',\n",
       " 'Fraser',\n",
       " 'Fraser',\n",
       " 'Fraser',\n",
       " 'Fraser',\n",
       " 'yeah',\n",
       " '',\n",
       " 'a',\n",
       " 'fly',\n",
       " 'fly',\n",
       " 'Mommy',\n",
       " 'telephone',\n",
       " 'my',\n",
       " 'telephone',\n",
       " 'Mommy',\n",
       " 'no',\n",
       " 'man',\n",
       " 'man',\n",
       " 'more',\n",
       " 'cookie',\n",
       " 'block',\n",
       " 'broke',\n",
       " 'there',\n",
       " 'I',\n",
       " 'did',\n",
       " 'it',\n",
       " 'there',\n",
       " 'there',\n",
       " 'Fraser',\n",
       " 'baby',\n",
       " 'Mommy',\n",
       " 'read',\n",
       " 'a',\n",
       " 'stool',\n",
       " 'Fraser',\n",
       " 'Fraser',\n",
       " 'more',\n",
       " 'cookie',\n",
       " 'more',\n",
       " 'cookie',\n",
       " 'little',\n",
       " 'little',\n",
       " 'little',\n",
       " 'little',\n",
       " 'milk',\n",
       " 'milk',\n",
       " 'milk',\n",
       " 'milk',\n",
       " 'that',\n",
       " 'Fraser',\n",
       " 'water',\n",
       " 'oh',\n",
       " 'Fraser',\n",
       " 'bye',\n",
       " 'water',\n",
       " 'Fraser',\n",
       " 'water',\n",
       " 'Fraser',\n",
       " 'water',\n",
       " 'that',\n",
       " 'coffee',\n",
       " 'Fraser',\n",
       " 'coffee',\n",
       " 'down',\n",
       " 'down',\n",
       " 'cookie',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'a',\n",
       " 'fly',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'read',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'Racketyboom',\n",
       " 'hat',\n",
       " 'm',\n",
       " 'water',\n",
       " 'bottle',\n",
       " 'water',\n",
       " 'there',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'oh',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'oh',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'oh',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'no',\n",
       " 'eye',\n",
       " 'that',\n",
       " 'that',\n",
       " 'soldier',\n",
       " 'soldier',\n",
       " 'that',\n",
       " 'that',\n",
       " 'Fraser',\n",
       " 'hat',\n",
       " 'Eve',\n",
       " 'find',\n",
       " 'it',\n",
       " 'that',\n",
       " 'that',\n",
       " 'man',\n",
       " 'Eve',\n",
       " 'down',\n",
       " 'that',\n",
       " 'busy',\n",
       " 'busy',\n",
       " 'man',\n",
       " 'a',\n",
       " 'pencil',\n",
       " 'man',\n",
       " 'a',\n",
       " 'pencil',\n",
       " 'man',\n",
       " 'pencil',\n",
       " 'Eve',\n",
       " 'pencil',\n",
       " 'Eve',\n",
       " 'pencil',\n",
       " 'Eve',\n",
       " 'pencil',\n",
       " 'man',\n",
       " 'a',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'man',\n",
       " 'see',\n",
       " 'ya',\n",
       " 'read',\n",
       " 'read',\n",
       " 'Clip_clop',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'shoe',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'Mommy',\n",
       " 'book',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'read',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'read',\n",
       " 'read',\n",
       " 'choo_choo',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'choo_choo',\n",
       " 'choo_choo',\n",
       " 'choo_choo',\n",
       " 'choo_choo',\n",
       " 'mi',\n",
       " 'milk',\n",
       " 'cup',\n",
       " 'that',\n",
       " 'top',\n",
       " 'that',\n",
       " 'there',\n",
       " 'quack',\n",
       " 'quack',\n",
       " 'quack',\n",
       " 'quack',\n",
       " 'that',\n",
       " 'that',\n",
       " 'radio',\n",
       " 'hat',\n",
       " 'that',\n",
       " 'like',\n",
       " 'the',\n",
       " 'farm',\n",
       " 'cock_a_doodle_doo',\n",
       " 'book',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'Racketyboom',\n",
       " 'yeah',\n",
       " 'why',\n",
       " 'choo_choo',\n",
       " 'stool',\n",
       " 'stool',\n",
       " 'man',\n",
       " 'stool',\n",
       " 'that',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'm',\n",
       " 'cup',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Mommy',\n",
       " 'get',\n",
       " 'it',\n",
       " 'that',\n",
       " 'a',\n",
       " 'stool',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'good',\n",
       " 'girl',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'Neil',\n",
       " 'sit',\n",
       " 'that',\n",
       " 'Eve',\n",
       " 'seat',\n",
       " 'Eve',\n",
       " 'seat',\n",
       " 'Eve',\n",
       " 'seat',\n",
       " 'Sambo',\n",
       " 'Sambo',\n",
       " 'Sambo',\n",
       " 'read',\n",
       " 'why',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'head',\n",
       " 'Mommy',\n",
       " 'paper',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'Mommy',\n",
       " 'lie',\n",
       " 'down',\n",
       " 'stool',\n",
       " 'lie',\n",
       " 'down',\n",
       " 'stool',\n",
       " 'man',\n",
       " 'Eve',\n",
       " 'writing',\n",
       " 'write',\n",
       " 'too',\n",
       " 'Mommy',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'that',\n",
       " 'that',\n",
       " 'pencil',\n",
       " 'read',\n",
       " 'cheese',\n",
       " 'm',\n",
       " 'man',\n",
       " 'man',\n",
       " 'that',\n",
       " 'that',\n",
       " 'Mommy',\n",
       " 'soup',\n",
       " 'that',\n",
       " 'pudding',\n",
       " 'soup',\n",
       " 'Mommy',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'man',\n",
       " 'more',\n",
       " 'Mommy',\n",
       " 'more',\n",
       " 'pudding',\n",
       " 'more',\n",
       " 'pudding',\n",
       " 'more',\n",
       " 'pudding',\n",
       " 'hot',\n",
       " 'soup',\n",
       " 'man',\n",
       " 'tape',\n",
       " 'men',\n",
       " 'tape',\n",
       " 'tape',\n",
       " 'tape',\n",
       " 'tape',\n",
       " 'man',\n",
       " 'tape',\n",
       " 'm',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'down',\n",
       " 'no',\n",
       " 'Mommy',\n",
       " 'paper',\n",
       " 'down',\n",
       " 'down',\n",
       " 'down',\n",
       " 'that',\n",
       " 'that',\n",
       " 'man',\n",
       " 'no',\n",
       " 'no',\n",
       " 'car',\n",
       " 'coming',\n",
       " 'car',\n",
       " 'come',\n",
       " 'come',\n",
       " 'car',\n",
       " 'dolly',\n",
       " 'celery',\n",
       " 'dolly',\n",
       " 'celery',\n",
       " 'doll',\n",
       " 'eat',\n",
       " 'the',\n",
       " 'dolly',\n",
       " 'doll',\n",
       " 'eat',\n",
       " 'celery',\n",
       " 'man',\n",
       " 'man',\n",
       " 'doll',\n",
       " 'oh',\n",
       " 'fish',\n",
       " 'fish',\n",
       " 'there',\n",
       " 'dolly',\n",
       " 'dolly',\n",
       " 'no',\n",
       " 'man',\n",
       " 'have',\n",
       " 'it',\n",
       " 'have',\n",
       " 'dolly',\n",
       " 'dolly',\n",
       " 'find',\n",
       " 'it',\n",
       " 'find',\n",
       " 'it',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'find',\n",
       " 'it',\n",
       " 'find',\n",
       " 'it',\n",
       " 'a',\n",
       " 'shoe',\n",
       " 'dolly',\n",
       " 'shoe',\n",
       " 'there',\n",
       " 'look',\n",
       " 'that',\n",
       " 'spool',\n",
       " 'where',\n",
       " 'top',\n",
       " 'top',\n",
       " 'find',\n",
       " 'i',\n",
       " 'that',\n",
       " 'spoon',\n",
       " 'spoon',\n",
       " 'spoon',\n",
       " 'man',\n",
       " 'no',\n",
       " 'spoon',\n",
       " 'coffee',\n",
       " 'my',\n",
       " 'oh',\n",
       " 'thank',\n",
       " 'oh',\n",
       " 'thank_you',\n",
       " 'that',\n",
       " 'that',\n",
       " 'a',\n",
       " 'top',\n",
       " 'a',\n",
       " 'top',\n",
       " 'a',\n",
       " 'top',\n",
       " 'good',\n",
       " 'man',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'why',\n",
       " 'mouth',\n",
       " 'mouth',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'be',\n",
       " 'careful',\n",
       " 'careful',\n",
       " 'man',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'sure',\n",
       " 'man',\n",
       " 'no',\n",
       " 'sure',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'man',\n",
       " 'no',\n",
       " 'taste',\n",
       " 'it',\n",
       " 'change',\n",
       " 'your',\n",
       " 'record',\n",
       " 'a',\n",
       " 'top',\n",
       " 'drop',\n",
       " 'a',\n",
       " 'table',\n",
       " 'man',\n",
       " 'fireplace',\n",
       " 'telephone',\n",
       " 'hi',\n",
       " 'hi',\n",
       " 'hi',\n",
       " 'hi',\n",
       " 'hi',\n",
       " 'Granma',\n",
       " 'my',\n",
       " 'my',\n",
       " 'Mommy',\n",
       " 'that',\n",
       " 'yeah',\n",
       " 'kitchen',\n",
       " 'no',\n",
       " 'no',\n",
       " 'pencil',\n",
       " 'a',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'milk',\n",
       " 'milk',\n",
       " 'Cromer',\n",
       " 'Cromer',\n",
       " 'a',\n",
       " 'coffee',\n",
       " 'Fraser',\n",
       " 'coffee',\n",
       " 'pencil',\n",
       " 'Becky',\n",
       " 'Becky',\n",
       " 'arm',\n",
       " 'arm',\n",
       " 'arm',\n",
       " 'arm',\n",
       " 'arm',\n",
       " 'arm',\n",
       " 'Mommy',\n",
       " 'letter',\n",
       " 'pencil',\n",
       " 'look',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'head',\n",
       " 'yeah',\n",
       " 'I',\n",
       " 'fall',\n",
       " 'milk',\n",
       " 'milk',\n",
       " 'cup',\n",
       " 'a',\n",
       " 'cup',\n",
       " 'write',\n",
       " 'write',\n",
       " 'juice',\n",
       " 'pencil',\n",
       " 'an',\n",
       " 'juice',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'juice',\n",
       " 'more',\n",
       " 'juice',\n",
       " 'Mommy',\n",
       " 'juice',\n",
       " 'a',\n",
       " 'cough',\n",
       " 'yep',\n",
       " 'Mommy',\n",
       " 'stair',\n",
       " 'juice',\n",
       " 'um',\n",
       " 'juice',\n",
       " 'juice',\n",
       " 'do',\n",
       " 'it',\n",
       " 'man',\n",
       " 'banjo',\n",
       " 'yep',\n",
       " 'there',\n",
       " 'there',\n",
       " 'Sambo',\n",
       " 'coffee',\n",
       " 'Sambo',\n",
       " 'Sambo',\n",
       " 'down',\n",
       " 'down',\n",
       " 'finger',\n",
       " 'book',\n",
       " 'read',\n",
       " 'Mommy',\n",
       " 'more',\n",
       " 'more',\n",
       " 'later',\n",
       " 'may',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'may',\n",
       " 'paper',\n",
       " 'the',\n",
       " 'paper',\n",
       " 'de',\n",
       " 'the',\n",
       " 'paper',\n",
       " 'table',\n",
       " 'Mommy',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'my',\n",
       " 'my',\n",
       " 'my',\n",
       " 'Mommy',\n",
       " 'pencil',\n",
       " 'Mommy',\n",
       " 'Daddy',\n",
       " 'down',\n",
       " 'down',\n",
       " 'down',\n",
       " 'yep',\n",
       " 'Mommy',\n",
       " 'down',\n",
       " 'Mommy',\n",
       " 'down',\n",
       " 'come',\n",
       " 'down',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'coffee',\n",
       " 'coffee',\n",
       " 'that',\n",
       " 'that',\n",
       " 'sugar',\n",
       " 'have',\n",
       " 'it',\n",
       " 'look',\n",
       " 'fall',\n",
       " 'down',\n",
       " 'Mommy',\n",
       " 'turn',\n",
       " 'here',\n",
       " 'water',\n",
       " 'that',\n",
       " 'Becky',\n",
       " 'turn',\n",
       " 'bring',\n",
       " 'it',\n",
       " 'bring',\n",
       " 'it',\n",
       " 'bye',\n",
       " 'Becky',\n",
       " 'Becky',\n",
       " 'why',\n",
       " 'where',\n",
       " 'Rangy',\n",
       " 'pardon',\n",
       " 'me',\n",
       " 'stair',\n",
       " 'door',\n",
       " 'a',\n",
       " 'Becky',\n",
       " 'a',\n",
       " 'Becky',\n",
       " 'a',\n",
       " 'Becky',\n",
       " 'Mommy',\n",
       " 'sit',\n",
       " 'down',\n",
       " 'room',\n",
       " 'read',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'Cromer',\n",
       " 'pencil',\n",
       " 'that',\n",
       " 'pencil',\n",
       " 'letter',\n",
       " 'letter',\n",
       " 'letter',\n",
       " 'tam',\n",
       " 'some',\n",
       " 'more',\n",
       " 'want',\n",
       " 'Mommy',\n",
       " 'letter',\n",
       " 'Mommy',\n",
       " 'letter',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'Mommy',\n",
       " 'letter',\n",
       " 'Kathy',\n",
       " 'letter',\n",
       " 'Kathy',\n",
       " 'letter',\n",
       " 'no',\n",
       " 'no',\n",
       " 'a',\n",
       " 'little',\n",
       " 'pencil',\n",
       " 'that',\n",
       " 'man',\n",
       " 'man',\n",
       " 'that',\n",
       " 'sugar',\n",
       " 'sugar',\n",
       " 'that',\n",
       " 'sugar',\n",
       " 'sugar',\n",
       " 'coffee',\n",
       " 'that',\n",
       " 'that',\n",
       " 'sugar',\n",
       " 'coffee',\n",
       " 'coffee',\n",
       " 'coffee',\n",
       " 'coffee',\n",
       " 'that',\n",
       " 'sugar',\n",
       " 'sugar',\n",
       " 'the',\n",
       " 'king',\n",
       " 'bear',\n",
       " 'cat',\n",
       " 'sugar',\n",
       " 'fox',\n",
       " 'fox',\n",
       " 'fox',\n",
       " 'fox',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'fox',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'that',\n",
       " 'that',\n",
       " 'Jack',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'a',\n",
       " 'lady',\n",
       " 'a',\n",
       " 'lady',\n",
       " 'a',\n",
       " 'lady',\n",
       " 'yep',\n",
       " 'that',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'letter',\n",
       " 'paper',\n",
       " 'paper',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'tiger',\n",
       " 'fox',\n",
       " 'Dumpty',\n",
       " 'a',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'that',\n",
       " 'Dumpty_Dumpty',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'that',\n",
       " 'kitty',\n",
       " 'kitty',\n",
       " 'kitty',\n",
       " 'that',\n",
       " 'that',\n",
       " 'fox',\n",
       " 'that',\n",
       " 'Dumpty',\n",
       " 'that',\n",
       " 'that',\n",
       " 'Dumpty',\n",
       " 'a',\n",
       " 'Dumpty',\n",
       " 'Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'a',\n",
       " 'milk',\n",
       " 'a',\n",
       " 'milk',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'pencil',\n",
       " 'pencil',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'eye',\n",
       " 'horsie',\n",
       " 'eye',\n",
       " 'horsie',\n",
       " 'eye',\n",
       " 'horsie',\n",
       " 'eye',\n",
       " 'horsie',\n",
       " 'eye',\n",
       " 'horsie',\n",
       " 'arm',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'horsie',\n",
       " 'a',\n",
       " 'horsie',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'Humpty_Dumpty',\n",
       " 'that',\n",
       " 'that',\n",
       " 'a',\n",
       " 'stool',\n",
       " 'a',\n",
       " 'stool',\n",
       " 'oh',\n",
       " 'fish',\n",
       " 'oh',\n",
       " 'fish',\n",
       " 'Humpty_Dumpty',\n",
       " 'fish',\n",
       " 'swimming',\n",
       " 'fish',\n",
       " 'are',\n",
       " 'swimming',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'briefcase',\n",
       " 'briefcase',\n",
       " 'that',\n",
       " 'Cromer',\n",
       " 'briefcase',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'horsie',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'Fraser',\n",
       " 'briefcase',\n",
       " 'banjo',\n",
       " 'banjo',\n",
       " 'Papa',\n",
       " 'banjo',\n",
       " 'table',\n",
       " 'on',\n",
       " 'wall',\n",
       " 'yep',\n",
       " 'laiby',\n",
       " 'sing',\n",
       " 'finger',\n",
       " 'stuck',\n",
       " 'finger',\n",
       " 'stuck',\n",
       " 'finger',\n",
       " 'stuck',\n",
       " 'finger',\n",
       " 'stuck',\n",
       " 'finger',\n",
       " 'stuck',\n",
       " 'yep',\n",
       " 'stand',\n",
       " 'dance',\n",
       " 'I',\n",
       " 'dance',\n",
       " 'bird',\n",
       " 'bird',\n",
       " 'bird',\n",
       " 'light',\n",
       " 'oh',\n",
       " 'light',\n",
       " 'Eve',\n",
       " 'light',\n",
       " 'Rangy',\n",
       " 'op',\n",
       " 'op',\n",
       " 'op',\n",
       " 'op',\n",
       " 'op',\n",
       " 'op',\n",
       " 'step',\n",
       " 'Mommy',\n",
       " 'step',\n",
       " 'play',\n",
       " 'step',\n",
       " 'step',\n",
       " 'step',\n",
       " 'a',\n",
       " 'step',\n",
       " 'Becky',\n",
       " 'why',\n",
       " 'Rangy',\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save emi variables (contingency matrix and N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 14975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMI SHOULD BE 6.4158670938439775\n",
    "# -3.776855083546411\n",
    "# 0.00644268190358682\n",
    "# 0.02389203129531103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m(1173)\u001b[0;36mresize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1171 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1172 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1173 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1174 \u001b[0;31m    \"\"\"\n",
      "\u001b[0m\u001b[0;32m   1175 \u001b[0;31m    \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/childes_mi_project/childes_mi/information_theory/mutual_information.py\u001b[0m(230)\u001b[0;36mMI_from_distributions\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    228 \u001b[0;31m                \u001b[0;31m#emi = expected_mutual_information(C, n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    229 \u001b[0;31m                \u001b[0memi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memi_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 230 \u001b[0;31m                \u001b[0;31m#emi2 = expected_mutual_information(C, n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    231 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, emi2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    232 \u001b[0;31m                return (\n",
      "\u001b[0m\n",
      "ipdb> emi\n",
      "-3.776855083546411\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for external emi cython: 2/100 [03:17<2:41:37, 98.95s/it]\n",
    "#for no external emi ~ 9 minutes (~4x slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:06:38.408014Z",
     "start_time": "2020-01-08T06:06:37.462947Z"
    }
   },
   "outputs": [],
   "source": [
    "mi_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:06:38.501972Z",
     "start_time": "2020-01-08T06:06:38.412990Z"
    }
   },
   "outputs": [],
   "source": [
    "#mi_df.to_pickle(DATA_DIR / \"mi\" / \"childes_mi.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:07:01.416196Z",
     "start_time": "2020-01-08T06:07:00.511399Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "for idx, row in mi_df.iterrows():\n",
    "    MI = row.MI-row.shuff_MI\n",
    "    MI_var = row.MI_var\n",
    "    #ax.scatter(distances, row.MI-row.shuff_MI)\n",
    "    ax.plot(distances, row.MI-row.shuff_MI, label = '{}-{} years'.format(row.age_cohort_low, row.age_cohort_high), lw=3)\n",
    "    #ax.fill_between(distances, MI-MI_var, MI+MI_var, alpha = 0.05)\n",
    "ax.legend()\n",
    "ax.set_ylim([1e-3,5])\n",
    "ax.set_xlim([1, 100])\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T06:09:04.929511Z",
     "start_time": "2020-01-08T06:09:01.306450Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(age_cohorts), figsize=(4*len(age_cohorts),4))\n",
    "for idx, row in mi_df.iterrows():\n",
    "\n",
    "    MI = row.MI-row.shuff_MI\n",
    "    MI_var = row.MI_var\n",
    "    #ax.scatter(distances, row.MI-row.shuff_MI)\n",
    "    ax = axs[np.array([i[0] for i in age_cohorts]) == row.age_cohort_low][0]\n",
    "    #ax.plot(distances, row.MI-row.shuff_MI, label = '{}-{} years'.format(row.age_cohort_low, row.age_cohort_high), lw=3)\n",
    "    ax.scatter(distances, row.MI-row.shuff_MI, s=10, color='k')\n",
    "    ax.fill_between(distances, MI-MI_var, MI+MI_var, alpha = 0.25, color= 'k')\n",
    "    #ax.legend()\n",
    "    ax.set_title('{}-{} years'.format(row.age_cohort_low, row.age_cohort_high))\n",
    "    ax.set_ylim([1e-3,5])\n",
    "    ax.set_xlim([1, 100])\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
